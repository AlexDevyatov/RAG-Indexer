#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –ø–æ–∏—Å–∫–∞ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ RAG.
–í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Ollama.
"""

import argparse
import json
import sys
from pathlib import Path
from typing import List, Dict, Optional

import numpy as np
import requests

from utils.embedder import DeepSeekEmbedder


# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Ollama
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "llama3.2"


def load_index(index_dir: str):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
    
    Args:
        index_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–Ω–¥–µ–∫—Å–æ–º
        
    Returns:
        –∫–æ—Ä—Ç–µ–∂ (faiss_index, metadata)
    """
    try:
        import faiss
    except ImportError:
        raise ImportError("üì¶ –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ FAISS: pip install faiss-cpu")
    
    index_path = Path(index_dir)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
    faiss_file = index_path / "faiss.index"
    if not faiss_file.exists():
        raise FileNotFoundError(f"‚ùå –ò–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {faiss_file}")
    
    index = faiss.read_index(str(faiss_file))
    print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω –∏–Ω–¥–µ–∫—Å: {index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata_file = index_path / "metadata.json"
    if not metadata_file.exists():
        raise FileNotFoundError(f"‚ùå –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {metadata_file}")
    
    with open(metadata_file, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    print(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(metadata)} –∑–∞–ø–∏—Å–µ–π")
    
    return index, metadata


def search_index(
    query: str,
    index,
    metadata: List[Dict],
    embedder: DeepSeekEmbedder,
    top_k: int = 3
) -> List[Dict]:
    """
    –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É.
    
    Args:
        query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        index: FAISS –∏–Ω–¥–µ–∫—Å
        metadata: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–æ–≤
        embedder: —ç–º–±–µ–¥–¥–µ—Ä
        top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        —Å–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –¥–∏—Å—Ç–∞–Ω—Ü–∏—è–º–∏
    """
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
    print(f"üîç –ü–æ–∏—Å–∫: \"{query}\"")
    query_embedding = embedder.embed_single(query)
    query_vector = np.array([query_embedding]).astype(np.float32)
    
    # –ü–æ–∏—Å–∫ –≤ –∏–Ω–¥–µ–∫—Å–µ
    distances, indices = index.search(query_vector, top_k)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results = []
    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
        if idx < len(metadata):
            result = metadata[idx].copy()
            result["distance"] = float(dist)
            result["rank"] = i + 1
            results.append(result)
    
    return results


def generate_response_ollama(
    query: str,
    context_chunks: List[Dict],
    model: str = OLLAMA_MODEL,
    stream: bool = True
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Ollama —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    
    Args:
        query: –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        context_chunks: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
        model: –º–æ–¥–µ–ª—å Ollama
        stream: –ø–æ—Ç–æ–∫–æ–≤—ã–π –≤—ã–≤–æ–¥
        
    Returns:
        —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
    """
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    context_parts = []
    for chunk in context_chunks:
        source = Path(chunk["source"]).name
        context_parts.append(f"[–ò—Å—Ç–æ—á–Ω–∏–∫: {source}]\n{chunk['text']}")
    
    context = "\n\n---\n\n".join(context_parts)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
    system_prompt = """–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.

–ü—Ä–∞–≤–∏–ª–∞:
1. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º
3. –£–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –µ—Å–ª–∏ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ
4. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
5. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º"""

    user_prompt = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

---

–í–æ–ø—Ä–æ—Å: {query}

–û—Ç–≤–µ—Ç:"""

    # –ó–∞–ø—Ä–æ—Å –∫ Ollama
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "stream": stream
    }
    
    try:
        response = requests.post(OLLAMA_URL, json=payload, stream=stream, timeout=120)
        response.raise_for_status()
    except requests.exceptions.ConnectionError:
        raise ConnectionError(
            f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama ({OLLAMA_URL})\n"
            "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω–∞: ollama serve"
        )
    except requests.exceptions.Timeout:
        raise TimeoutError("‚ùå –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Ollama")
    except requests.exceptions.HTTPError as e:
        raise RuntimeError(f"‚ùå –û—à–∏–±–∫–∞ Ollama: {e}")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç
    if stream:
        full_response = ""
        print("\nü§ñ –û—Ç–≤–µ—Ç:\n")
        
        for line in response.iter_lines():
            if line:
                try:
                    data = json.loads(line)
                    if "message" in data and "content" in data["message"]:
                        chunk = data["message"]["content"]
                        full_response += chunk
                        print(chunk, end="", flush=True)
                    
                    if data.get("done", False):
                        break
                except json.JSONDecodeError:
                    continue
        
        print("\n")
        return full_response
    else:
        data = response.json()
        return data["message"]["content"]


def query_rag(
    query: str,
    index_dir: str,
    top_k: int = 3,
    model: str = OLLAMA_MODEL,
    show_sources: bool = True
):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è RAG –∑–∞–ø—Ä–æ—Å–∞.
    
    Args:
        query: –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        index_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–Ω–¥–µ–∫—Å–æ–º
        top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        model: –º–æ–¥–µ–ª—å Ollama
        show_sources: –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    """
    print("=" * 60)
    print("üîé RAG –ó–ê–ü–†–û–°")
    print("=" * 60)
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å
    print(f"\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑: {index_dir}")
    index, metadata = load_index(index_dir)
    
    # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–µ—Ä
    embedder = DeepSeekEmbedder()
    
    # 3. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    print(f"\nüîç –ü–æ–∏—Å–∫ —Ç–æ–ø-{top_k} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
    results = search_index(query, index, metadata, embedder, top_k)
    
    if not results:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤!")
        return
    
    # 4. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    if show_sources:
        print(f"\nüìö –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:")
        for result in results:
            source = Path(result["source"]).name
            dist = result["distance"]
            text_preview = result["text"][:100].replace("\n", " ")
            print(f"  #{result['rank']} [{source}] (dist: {dist:.4f})")
            print(f"      \"{text_preview}...\"")
    
    # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    print(f"\nüß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ {model}...")
    
    response = generate_response_ollama(query, results, model)
    
    # –ò—Ç–æ–≥
    print("=" * 60)
    print("‚úÖ –ó–ê–ü–†–û–° –í–´–ü–û–õ–ù–ï–ù")
    print("=" * 60)
    
    return response


def interactive_mode(index_dir: str, top_k: int = 3, model: str = OLLAMA_MODEL):
    """
    –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
    
    Args:
        index_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–Ω–¥–µ–∫—Å–æ–º
        top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        model: –º–æ–¥–µ–ª—å Ollama
    """
    print("=" * 60)
    print("üéØ –ò–ù–¢–ï–†–ê–ö–¢–ò–í–ù–´–ô –†–ï–ñ–ò–ú RAG")
    print("=" * 60)
    print("–í–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥' –∏–ª–∏ 'exit' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è\n")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å –æ–¥–∏–Ω —Ä–∞–∑
    index, metadata = load_index(index_dir)
    embedder = DeepSeekEmbedder()
    
    while True:
        try:
            query = input("\n‚ùì –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
            
            if not query:
                continue
            
            if query.lower() in ("–≤—ã—Ö–æ–¥", "exit", "quit", "q"):
                print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            # –ü–æ–∏—Å–∫
            results = search_index(query, index, metadata, embedder, top_k)
            
            if not results:
                print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤!")
                continue
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            generate_response_ollama(query, results, model)
            
        except KeyboardInterrupt:
            print("\n\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏."""
    parser = argparse.ArgumentParser(
        description="–ü–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —á–µ—Ä–µ–∑ RAG",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  python query.py --index my_index --query "–ö–∞–∫ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–µ–∫—Ç?"
  python query.py --index my_index --query "–û–ø–∏—à–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É" --top-k 5
  python query.py --index my_index --interactive
        """
    )
    
    parser.add_argument(
        "--index", "-i",
        required=True,
        help="–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏–Ω–¥–µ–∫—Å–æ–º"
    )
    
    parser.add_argument(
        "--query", "-q",
        help="–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å"
    )
    
    parser.add_argument(
        "--top-k", "-k",
        type=int,
        default=3,
        help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 3)"
    )
    
    parser.add_argument(
        "--model", "-m",
        default=OLLAMA_MODEL,
        help=f"–ú–æ–¥–µ–ª—å Ollama (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: {OLLAMA_MODEL})"
    )
    
    parser.add_argument(
        "--interactive",
        action="store_true",
        help="–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º"
    )
    
    parser.add_argument(
        "--no-sources",
        action="store_true",
        help="–ù–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏"
    )
    
    args = parser.parse_args()
    
    try:
        if args.interactive:
            interactive_mode(args.index, args.top_k, args.model)
        elif args.query:
            query_rag(
                query=args.query,
                index_dir=args.index,
                top_k=args.top_k,
                model=args.model,
                show_sources=not args.no_sources
            )
        else:
            parser.error("–£–∫–∞–∂–∏—Ç–µ --query –∏–ª–∏ --interactive")
            
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

#!/usr/bin/env python3
"""
–í–µ–±-—Å–µ—Ä–≤–µ—Ä –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã.
–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç API –∏ –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –ø–æ–∏—Å–∫–∞.
"""

import os
import json
import uuid
import shutil
import time
import logging
from pathlib import Path
from typing import List, Optional
from threading import Lock
from datetime import datetime

from flask import Flask, request, jsonify, render_template, send_from_directory, Response, stream_with_context
from flask_cors import CORS
from werkzeug.utils import secure_filename
import numpy as np

from utils.parser import parse_file, get_file_type
from utils.chunker import chunk_document
from utils.embedder import OllamaEmbedder

# === –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø ===
LOG_FOLDER = "logs"
Path(LOG_FOLDER).mkdir(parents=True, exist_ok=True)

# –§–æ—Ä–º–∞—Ç –ª–æ–≥–æ–≤
log_format = logging.Formatter(
    '%(asctime)s | %(levelname)-8s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# –õ–æ–≥–≥–µ—Ä –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
logger = logging.getLogger('rag_indexer')
logger.setLevel(logging.DEBUG)

# –§–∞–π–ª–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
file_handler = logging.FileHandler(
    f'{LOG_FOLDER}/app.log',
    encoding='utf-8'
)
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(log_format)

# –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(log_format)

logger.addHandler(file_handler)
logger.addHandler(console_handler)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
UPLOAD_FOLDER = "uploads"
INDEX_FOLDER = "index_data"
ALLOWED_EXTENSIONS = {'pdf', 'docx', 'txt', 'md', 'py', 'js', 'ts', 'java', 'cpp', 'c', 'go', 'rs', 'json', 'yaml', 'yml'}

app = Flask(__name__, static_folder='static', template_folder='templates')
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB –º–∞–∫—Å
CORS(app)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞
faiss_index = None
metadata = []
embedder = None

# === –ü–†–û–ì–†–ï–°–° –ò–ù–î–ï–ö–°–ê–¶–ò–ò ===
# –≠—Ç–∞–ø—ã: documents, parsing, chunking, embedding, faiss
# –°—Ç–∞—Ç—É—Å—ã: pending, processing, completed, error
indexing_status = {
    "active": False,
    "steps": {
        "documents": {"status": "pending", "message": ""},
        "parsing": {"status": "pending", "message": ""},
        "chunking": {"status": "pending", "message": ""},
        "embedding": {"status": "pending", "message": ""},
        "faiss": {"status": "pending", "message": ""}
    },
    "error": None,
    "current_file": "",
    "total_files": 0,
    "processed_files": 0
}
indexing_lock = Lock()


def reset_indexing_status():
    """–°–±—Ä–æ—Å —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
    global indexing_status
    with indexing_lock:
        indexing_status = {
            "active": False,
            "steps": {
                "documents": {"status": "pending", "message": ""},
                "parsing": {"status": "pending", "message": ""},
                "chunking": {"status": "pending", "message": ""},
                "embedding": {"status": "pending", "message": ""},
                "faiss": {"status": "pending", "message": ""}
            },
            "error": None,
            "current_file": "",
            "total_files": 0,
            "processed_files": 0
        }


def update_step_status(step: str, status: str, message: str = ""):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —ç—Ç–∞–ø–∞."""
    global indexing_status
    with indexing_lock:
        if step in indexing_status["steps"]:
            indexing_status["steps"][step]["status"] = status
            indexing_status["steps"][step]["message"] = message


def set_indexing_error(error_message: str, failed_step: str = None):
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—à–∏–±–∫–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
    global indexing_status
    with indexing_lock:
        indexing_status["error"] = error_message
        indexing_status["active"] = False
        if failed_step and failed_step in indexing_status["steps"]:
            indexing_status["steps"][failed_step]["status"] = "error"
            indexing_status["steps"][failed_step]["message"] = error_message


def allowed_file(filename: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–ø—É—Å—Ç–∏–º–æ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–∞."""
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS


def init_embedder():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–µ—Ä–∞."""
    global embedder
    if embedder is None:
        try:
            embedder = OllamaEmbedder()
            print("‚úÖ –≠–º–±–µ–¥–¥–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–µ—Ä–∞: {e}")
            raise


def load_index():
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞."""
    global faiss_index, metadata
    
    try:
        import faiss
    except ImportError:
        print("‚ùå FAISS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        return False
    
    index_path = Path(INDEX_FOLDER)
    faiss_file = index_path / "faiss.index"
    metadata_file = index_path / "metadata.json"
    
    if faiss_file.exists() and metadata_file.exists():
        faiss_index = faiss.read_index(str(faiss_file))
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω –∏–Ω–¥–µ–∫—Å: {faiss_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
        return True
    
    return False


def save_index():
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞ –¥–∏—Å–∫."""
    global faiss_index, metadata
    
    try:
        import faiss
    except ImportError:
        return False
    
    index_path = Path(INDEX_FOLDER)
    index_path.mkdir(parents=True, exist_ok=True)
    
    if faiss_index is not None:
        faiss.write_index(faiss_index, str(index_path / "faiss.index"))
        with open(index_path / "metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        return True
    
    return False


def create_or_update_index(texts: List[str], sources: List[dict]):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞."""
    global faiss_index, metadata, embedder
    
    try:
        import faiss
    except ImportError:
        raise ImportError("FAISS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    init_embedder()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    update_step_status("embedding", "processing", f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(texts)} —á–∞–Ω–∫–æ–≤...")
    
    logger.info(f"üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(texts)} —á–∞–Ω–∫–æ–≤...")
    embed_start = time.time()
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    embeddings = embedder.embed_texts(texts, show_progress=False)
    embeddings_array = np.array(embeddings).astype(np.float32)
    
    embed_time = time.time() - embed_start
    logger.info(f"‚è±Ô∏è  –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –∑–∞ {embed_time:.2f} —Å–µ–∫ ({len(texts)/embed_time:.1f} —á–∞–Ω–∫–æ–≤/—Å–µ–∫)")
    
    update_step_status("embedding", "completed", f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(texts)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ FAISS
    update_step_status("faiss", "processing", "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ FAISS –∏–Ω–¥–µ–∫—Å...")
    
    dimension = embeddings_array.shape[1]
    logger.debug(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {dimension}")
    
    # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç - —Å–æ–∑–¥–∞—ë–º
    if faiss_index is None:
        faiss_index = faiss.IndexFlatL2(dimension)
        logger.info(f"üì¶ –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π FAISS –∏–Ω–¥–µ–∫—Å (dim={dimension})")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
    start_id = faiss_index.ntotal
    faiss_index.add(embeddings_array)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    for i, (text, source) in enumerate(zip(texts, sources)):
        metadata.append({
            "id": start_id + i,
            "text": text,
            "source": source["source"],
            "filename": source["filename"],
            "chunk_index": source.get("chunk_index", 0)
        })
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    save_index()
    
    logger.info(f"üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {faiss_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
    update_step_status("faiss", "completed", f"–ò–Ω–¥–µ–∫—Å —Å–æ–¥–µ—Ä–∂–∏—Ç {faiss_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
    
    return len(texts)


def search_index(query: str, top_k: int = 3) -> List[dict]:
    """–ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É."""
    global faiss_index, metadata, embedder
    
    if faiss_index is None or faiss_index.ntotal == 0:
        return []
    
    init_embedder()
    
    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
    query_embedding = embedder.embed_single(query)
    query_vector = np.array([query_embedding]).astype(np.float32)
    
    # –ü–æ–∏—Å–∫
    distances, indices = faiss_index.search(query_vector, min(top_k, faiss_index.ntotal))
    
    results = []
    for dist, idx in zip(distances[0], indices[0]):
        if idx < len(metadata):
            result = metadata[idx].copy()
            result["distance"] = float(dist)
            results.append(result)
    
    return results


def generate_answer(query: str, context_chunks: List[dict]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ DeepSeek API."""
    import requests
    from dotenv import load_dotenv
    
    load_dotenv()
    api_key = os.getenv("DEEPSEEK_API_KEY")
    
    if not api_key:
        logger.error("DEEPSEEK_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env")
        return "‚ùå –û—à–∏–±–∫–∞: DEEPSEEK_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env"
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    context_parts = []
    for chunk in context_chunks:
        context_parts.append(f"[{chunk['filename']}]\n{chunk['text']}")
    
    context = "\n\n---\n\n".join(context_parts)
    
    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    context_chars = len(context)
    context_words = len(context.split())
    logger.info(f"üìä –ö–æ–Ω—Ç–µ–∫—Å—Ç: {len(context_chunks)} —á–∞–Ω–∫–æ–≤, {context_chars} —Å–∏–º–≤–æ–ª–æ–≤, ~{context_words} —Å–ª–æ–≤")
    logger.debug(f"–í–æ–ø—Ä–æ—Å: {query[:100]}...")
    
    system_prompt = """–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."""

    user_prompt = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

---

–í–æ–ø—Ä–æ—Å: {query}"""

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω)
    estimated_tokens = (len(system_prompt) + len(user_prompt)) // 4
    logger.info(f"üìù –ü—Ä–∏–º–µ—Ä–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: ~{estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤")

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "deepseek-chat",
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 2048
    }
    
    try:
        logger.info("üöÄ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ DeepSeek API...")
        start_time = time.time()
        
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=120  # –£–≤–µ–ª–∏—á–µ–Ω —Ç–∞–π–º–∞—É—Ç –¥–æ 120 —Å–µ–∫—É–Ω–¥
        )
        
        elapsed_time = time.time() - start_time
        logger.info(f"‚è±Ô∏è  –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ DeepSeek API: {elapsed_time:.2f} —Å–µ–∫")
        
        if response.status_code == 401:
            logger.error("–ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á DeepSeek")
            return "‚ùå –û—à–∏–±–∫–∞: –ù–µ–≤–µ—Ä–Ω—ã–π API –∫–ª—é—á DeepSeek"
        
        if response.status_code == 429:
            logger.error("–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ DeepSeek API")
            return "‚ùå –û—à–∏–±–∫–∞: –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ DeepSeek API"
        
        response.raise_for_status()
        data = response.json()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
        if 'usage' in data:
            usage = data['usage']
            logger.info(f"üìà –¢–æ–∫–µ–Ω—ã: prompt={usage.get('prompt_tokens', '?')}, "
                       f"completion={usage.get('completion_tokens', '?')}, "
                       f"total={usage.get('total_tokens', '?')}")
        
        answer = data["choices"][0]["message"]["content"]
        logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω: {len(answer)} —Å–∏–º–≤–æ–ª–æ–≤")
        logger.debug(f"–û—Ç–≤–µ—Ç: {answer[:200]}...")
        
        return answer
        
    except requests.exceptions.ConnectionError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ DeepSeek API: {e}")
        return "‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ DeepSeek API"
    except requests.exceptions.Timeout:
        elapsed_time = time.time() - start_time
        logger.error(f"‚ùå –¢–ê–ô–ú–ê–£–¢ DeepSeek API –ø–æ—Å–ª–µ {elapsed_time:.2f} —Å–µ–∫! "
                    f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context_chars} —Å–∏–º–≤–æ–ª–æ–≤, ~{estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
        return "‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–ø—Ä–æ—Å—Ç–∏—Ç—å –∑–∞–ø—Ä–æ—Å."
    except requests.exceptions.HTTPError as e:
        logger.error(f"HTTP –æ—à–∏–±–∫–∞: {e.response.status_code} - {e.response.text}")
        return f"‚ùå –û—à–∏–±–∫–∞ HTTP: {e.response.status_code} - {e.response.text}"
    except Exception as e:
        logger.exception(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        return f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"


# === API –≠–ù–î–ü–û–ò–ù–¢–´ ===

@app.route('/')
def index():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞."""
    return render_template('index.html')


@app.route('/api/progress')
def progress_stream():
    """SSE —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
    def generate():
        while True:
            with indexing_lock:
                data = json.dumps(indexing_status)
            yield f"data: {data}\n\n"
            
            # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–µ –∞–∫—Ç–∏–≤–Ω–∞ –∏ –Ω–µ—Ç –æ—à–∏–±–∫–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∂–µ
            with indexing_lock:
                is_active = indexing_status["active"]
            
            if is_active:
                time.sleep(0.5)  # –ß–∞—â–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ –≤—Ä–µ–º—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            else:
                time.sleep(2)  # –†–µ–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤ —Ä–µ–∂–∏–º–µ –æ–∂–∏–¥–∞–Ω–∏—è
    
    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
            'X-Accel-Buffering': 'no'
        }
    )


@app.route('/api/upload', methods=['POST'])
def upload_files():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤."""
    global indexing_status
    
    if 'files' not in request.files:
        return jsonify({"error": "–§–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"}), 400
    
    files = request.files.getlist('files')
    
    if not files or all(f.filename == '' for f in files):
        return jsonify({"error": "–§–∞–π–ª—ã –Ω–µ –≤—ã–±—Ä–∞–Ω—ã"}), 400
    
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏ –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º —Å—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    reset_indexing_status()
    with indexing_lock:
        indexing_status["active"] = True
        indexing_status["total_files"] = len([f for f in files if f and f.filename and allowed_file(f.filename)])
    
    # –≠—Ç–∞–ø 1: –î–æ–∫—É–º–µ–Ω—Ç—ã
    update_step_status("documents", "processing", f"–ü–æ–ª—É—á–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤")
    
    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è –∑–∞–≥—Ä—É–∑–æ–∫
    upload_path = Path(UPLOAD_FOLDER)
    upload_path.mkdir(parents=True, exist_ok=True)
    
    processed_files = []
    all_chunks = []
    all_sources = []
    
    update_step_status("documents", "completed", f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤")
    
    # –≠—Ç–∞–ø 2: –ü–∞—Ä—Å–∏–Ω–≥
    update_step_status("parsing", "processing", "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    
    for file in files:
        if file and file.filename and allowed_file(file.filename):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
            filename = secure_filename(file.filename)
            unique_name = f"{uuid.uuid4().hex[:8]}_{filename}"
            filepath = upload_path / unique_name
            file.save(str(filepath))
            
            with indexing_lock:
                indexing_status["current_file"] = filename
            
            try:
                # –ü–∞—Ä—Å–∏–º —Ñ–∞–π–ª
                text, file_type = parse_file(str(filepath))
                
                with indexing_lock:
                    indexing_status["processed_files"] += 1
                
                update_step_status("parsing", "processing", f"–û–±—Ä–∞–±–æ—Ç–∞–Ω: {filename}")
                
                # –≠—Ç–∞–ø 3: –ß–∞–Ω–∫–∏–Ω–≥ (–æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞)
                update_step_status("chunking", "processing", f"–†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏: {filename}")
                
                # –ß–∞–Ω–∫–∏–Ω–≥
                chunks = chunk_document(text, str(filepath), chunk_size=512, chunk_overlap=50)
                
                for chunk in chunks:
                    all_chunks.append(chunk["text"])
                    all_sources.append({
                        "source": str(filepath),
                        "filename": filename,
                        "chunk_index": chunk["chunk_index"]
                    })
                
                processed_files.append({
                    "filename": filename,
                    "chunks": len(chunks),
                    "type": file_type
                })
                
            except Exception as e:
                processed_files.append({
                    "filename": filename,
                    "error": str(e)
                })
                set_indexing_error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {filename}: {str(e)}", "parsing")
    
    update_step_status("parsing", "completed", f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(processed_files)} —Ñ–∞–π–ª–æ–≤")
    update_step_status("chunking", "completed", f"–°–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")
    
    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤—Å–µ —á–∞–Ω–∫–∏
    if all_chunks:
        try:
            indexed_count = create_or_update_index(all_chunks, all_sources)
            
            # –ó–∞–≤–µ—Ä—à–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
            with indexing_lock:
                indexing_status["active"] = False
                indexing_status["current_file"] = ""
            
            return jsonify({
                "success": True,
                "files": processed_files,
                "indexed_chunks": indexed_count,
                "total_vectors": faiss_index.ntotal if faiss_index else 0
            })
        except Exception as e:
            set_indexing_error(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {str(e)}", "embedding")
            return jsonify({
                "error": f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {str(e)}",
                "files": processed_files
            }), 500
    
    # –ó–∞–≤–µ—Ä—à–∞–µ–º –±–µ–∑ —á–∞–Ω–∫–æ–≤
    with indexing_lock:
        indexing_status["active"] = False
    
    return jsonify({
        "success": True,
        "files": processed_files,
        "indexed_chunks": 0
    })


@app.route('/api/search', methods=['POST'])
def search():
    """–ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É."""
    data = request.get_json()
    
    if not data or 'query' not in data:
        return jsonify({"error": "–ó–∞–ø—Ä–æ—Å –Ω–µ —É–∫–∞–∑–∞–Ω"}), 400
    
    query = data['query']
    top_k = data.get('top_k', 3)
    
    try:
        results = search_index(query, top_k)
        return jsonify({
            "success": True,
            "query": query,
            "results": results
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500


@app.route('/api/ask', methods=['POST'])
def ask():
    """RAG –∑–∞–ø—Ä–æ—Å: –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞."""
    data = request.get_json()
    
    if not data or 'query' not in data:
        return jsonify({"error": "–í–æ–ø—Ä–æ—Å –Ω–µ —É–∫–∞–∑–∞–Ω"}), 400
    
    query = data['query']
    top_k = data.get('top_k', 3)
    
    logger.info("=" * 50)
    logger.info(f"üîç –ù–æ–≤—ã–π RAG-–∑–∞–ø—Ä–æ—Å: '{query[:80]}...' (top_k={top_k})")
    request_start = time.time()
    
    try:
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        search_start = time.time()
        results = search_index(query, top_k)
        search_time = time.time() - search_start
        logger.info(f"‚è±Ô∏è  –ü–æ–∏—Å–∫ –≤ FAISS: {search_time:.3f} —Å–µ–∫, –Ω–∞–π–¥–µ–Ω–æ {len(results)} —á–∞–Ω–∫–æ–≤")
        
        if not results:
            logger.warning("–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return jsonify({
                "success": True,
                "query": query,
                "answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –∏–Ω–¥–µ–∫—Å–µ –Ω–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.",
                "sources": []
            })
        
        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        for i, r in enumerate(results):
            logger.debug(f"  [{i+1}] {r['filename']} (distance: {r['distance']:.4f})")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        generation_start = time.time()
        answer = generate_answer(query, results)
        generation_time = time.time() - generation_start
        
        total_time = time.time() - request_start
        logger.info(f"‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è –∑–∞–ø—Ä–æ—Å–∞: {total_time:.2f} —Å–µ–∫ "
                   f"(–ø–æ–∏—Å–∫: {search_time:.3f}—Å, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {generation_time:.2f}—Å)")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ—Ç–≤–µ—Ç –æ—à–∏–±–∫—É —Ç–∞–π–º–∞—É—Ç–∞
        if "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è" in answer:
            logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç –ø–æ—Å–ª–µ {total_time:.2f} —Å–µ–∫")
            return jsonify({
                "error": "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–ø—Ä–æ—Å—Ç–∏—Ç—å –∑–∞–ø—Ä–æ—Å."
            }), 504
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—Ä—É–≥–∏–µ –æ—à–∏–±–∫–∏
        if answer.startswith("‚ùå"):
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {answer}")
            return jsonify({
                "error": answer.replace("‚ùå ", "")
            }), 500
        
        logger.info(f"‚úÖ –ó–∞–ø—Ä–æ—Å —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ {total_time:.2f} —Å–µ–∫")
        
        return jsonify({
            "success": True,
            "query": query,
            "answer": answer,
            "sources": [{"filename": r["filename"], "distance": r["distance"]} for r in results]
        })
        
    except requests.exceptions.Timeout:
        total_time = time.time() - request_start
        logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –ø–æ—Å–ª–µ {total_time:.2f} —Å–µ–∫")
        return jsonify({
            "error": "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–ø—Ä–æ—Å—Ç–∏—Ç—å –∑–∞–ø—Ä–æ—Å."
        }), 504
    except Exception as e:
        logger.exception(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ /api/ask: {e}")
        return jsonify({"error": f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"}), 500


@app.route('/api/stats', methods=['GET'])
def stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞."""
    return jsonify({
        "total_vectors": faiss_index.ntotal if faiss_index else 0,
        "total_documents": len(set(m.get("filename", "") for m in metadata)) if metadata else 0,
        "total_chunks": len(metadata)
    })


@app.route('/api/clear', methods=['POST'])
def clear_index():
    """–û—á–∏—Å—Ç–∫–∞ –∏–Ω–¥–µ–∫—Å–∞."""
    global faiss_index, metadata
    
    try:
        import faiss
        faiss_index = None
        metadata = []
        
        # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
        index_path = Path(INDEX_FOLDER)
        if index_path.exists():
            shutil.rmtree(index_path)
        
        upload_path = Path(UPLOAD_FOLDER)
        if upload_path.exists():
            shutil.rmtree(upload_path)
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        reset_indexing_status()
        
        return jsonify({"success": True, "message": "–ò–Ω–¥–µ–∫—Å –æ—á–∏—â–µ–Ω"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


if __name__ == '__main__':
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º requests –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ ask
    import requests
    
    # –°–æ–∑–¥–∞—ë–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    Path(UPLOAD_FOLDER).mkdir(parents=True, exist_ok=True)
    Path(INDEX_FOLDER).mkdir(parents=True, exist_ok=True)
    Path('templates').mkdir(parents=True, exist_ok=True)
    Path('static').mkdir(parents=True, exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
    load_index()
    
    logger.info("=" * 60)
    logger.info("üöÄ RAG WEB SERVER –ó–ê–ü–£–©–ï–ù")
    logger.info("=" * 60)
    logger.info(f"üìç URL: http://localhost:8001")
    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∏: {UPLOAD_FOLDER}/")
    logger.info(f"üóÇÔ∏è  –ò–Ω–¥–µ–∫—Å: {INDEX_FOLDER}/")
    logger.info(f"üìã –õ–æ–≥–∏: {LOG_FOLDER}/app.log")
    logger.info("=" * 60)
    
    print("=" * 60)
    print("üöÄ RAG WEB SERVER")
    print("=" * 60)
    print("üìç URL: http://localhost:8001")
    print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∏: uploads/")
    print("üóÇÔ∏è  –ò–Ω–¥–µ–∫—Å: index_data/")
    print(f"üìã –õ–æ–≥–∏: {LOG_FOLDER}/app.log")
    print("=" * 60)
    
    app.run(host='0.0.0.0', port=8001, debug=False)
